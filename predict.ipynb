{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df537d0-e5a8-444c-9dd4-aa305cf5e7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prediction Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56e5793-509f-4723-b692-8e53e5475b3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First insert the command line arguments as dbutils widget parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5782b091-abda-4284-9eb8-213db1dd1111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Data params ---\n",
    "dbutils.widgets.text(\"dataset\", \"system_1\")\n",
    "dbutils.widgets.text(\"eval_start\", \"0\")\n",
    "dbutils.widgets.text(\"eval_end\", \"None\")\n",
    "\n",
    "# --- Predict params ---\n",
    "dbutils.widgets.text(\"use_cuda\", \"True\")\n",
    "dbutils.widgets.text(\"threshold_type\", \"POT\")\n",
    "# If threshold_type is set to POT, these are the POT params\n",
    "dbutils.widgets.text(\"use_mov_av\", \"False\")\n",
    "dbutils.widgets.text(\"q\", \"0.001\")\n",
    "dbutils.widgets.text(\"level\", \"0.99\")\n",
    "dbutils.widgets.text(\"dynamic_pot\", \"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f250e7-70cd-4e8e-8ca9-d700703a4bf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd48262a-2865-4dfa-9305-3b7d35d86c98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from architecture import MTAD_GAT\n",
    "from model import Handler\n",
    "from utils import str2bool, str2type\n",
    "from utils import get_data, SlidingWindowDataset, create_data_loader, pot_threshold, get_run_id, json_to_numpy\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29be5f4-0323-4149-a1b1-36645a883ef8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get the parameters' values and fix them to the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca13ab1-18ae-4d94-aee6-6b63c4586801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = dbutils.widgets.get(\"dataset\")\n",
    "eval_start = int(dbutils.widgets.get(\"eval_start\"))\n",
    "eval_end = str2type(dbutils.widgets.get(\"eval_end\"))\n",
    "\n",
    "use_cuda = str2type(dbutils.widgets.get(\"use_cuda\"))\n",
    "threshold_type = dbutils.widgets.get(\"threshold_type\")\n",
    "use_mov_av = str2type(dbutils.widgets.get(\"use_mov_av\"))\n",
    "q = float(dbutils.widgets.get(\"q\"))\n",
    "level = float(dbutils.widgets.get(\"level\"))\n",
    "dynamic_pot = str2type(dbutils.widgets.get(\"dynamic_pot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c801a4a-db9f-4d4a-b9a9-5e8ee2ca4c05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Make sure the proper container (to draw data from) is mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9db19c-3fc3-48a7-8023-b415ed652962",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mount already exists.\n"
     ]
    }
   ],
   "source": [
    "# Checking if mount already exists\n",
    "mnts = dbutils.fs.mounts()\n",
    "mnt_exists = False\n",
    "for mount in mnts:\n",
    "    if mount.mountPoint == \"/mnt/datasets\":\n",
    "        mnt_exists = True\n",
    "\n",
    "if mnt_exists == False:\n",
    "    # Setup some parameters and keys\n",
    "    account_name = \"canopuslake\"\n",
    "    container = \"datasets\"\n",
    "\n",
    "    client_secret = dbutils.secrets.get(scope=\"vault_scope\", key=\"dbricks-to-lake-secret\")\n",
    "    client_id = dbutils.secrets.get(scope=\"vault_scope\", key=\"dbricks-to-lake-client-ID\")\n",
    "    tenant_id = dbutils.secrets.get(scope=\"vault_scope\", key=\"dbricks-to-lake-tenant-ID\")\n",
    "\n",
    "    # Define the connection configurations\n",
    "    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "          \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"}\n",
    "\n",
    "    # Command to mount the blob storage container locally\n",
    "    dbutils.fs.mount(\n",
    "    source = f\"abfss://{container}@{account_name}.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/datasets\",\n",
    "    extra_configs = configs)\n",
    "else:\n",
    "    print(\"Mount already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0363a908-922c-41f1-98a6-3e0643de25df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, run the inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56841c71-96ec-4220-ba2a-9d28c28d3994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model found in Production stage, using model from latest run for predictions.\nThe model's run ID is: 0dac450f8b2d4e5294070621318ae72b\nPredicting:\nThe size of the dataset is: 989 sample(s).\nCalculating scores on new data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/4 [00:00<?, ?it/s]\r 25%|██▌       | 1/4 [00:00<00:02,  1.35it/s]\r 50%|█████     | 2/4 [00:00<00:00,  2.44it/s]\r 75%|███████▌  | 3/4 [00:01<00:00,  3.36it/s]\r100%|██████████| 4/4 [00:01<00:00,  4.22it/s]\r100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scores from data used for training...\nRunning POT with q=0.001, level=0.99..\nInitial threshold : 0.7201882004737854\nNumber of peaks : 283\nGrimshaw maximum log-likelihood estimation ... [done]\n\tγ = -0.016250451967990354\n\tσ = 0.2116042329432291\n\tL = 161.10851155849537\nExtreme quantile (probability = 0.001): 1.197853647367851\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/989 [00:00<?, ?it/s]\r100%|██████████| 989/989 [00:00<00:00, 387787.85it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting anomalies based on POT-generated threshold - threshold value: 1.1978536473678507\nFinished.\n"
     ]
    }
   ],
   "source": [
    "# Get custom id for every run\n",
    "id = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "\n",
    "dataset = dataset\n",
    "\n",
    "experiment = mlflow.set_experiment(experiment_name=f\"/Experiments/{dataset}_inference\")\n",
    "exp_id = experiment.experiment_id\n",
    "\n",
    "with mlflow.start_run(experiment_id=exp_id, run_name=id):\n",
    "\n",
    "    # Get Production (or latest) model and run_id\n",
    "    model_uri = f\"models:/{dataset}_model/Production\"\n",
    "    try:\n",
    "        model = mlflow.pytorch.load_model(model_uri)\n",
    "        print(f\"Fetched {dataset}_model from Production for predictions.\")\n",
    "        # get corresponding run_id\n",
    "        client = mlflow.MlflowClient()\n",
    "        run_id = client.get_latest_versions(f\"{dataset}_model\", stages=[\"Production\"])[0].run_id\n",
    "    except mlflow.exceptions.MlflowException:\n",
    "        run_id = get_run_id(\"-1\", f\"/Experiments/{dataset}_training\")\n",
    "        model_uri = f\"runs:/{run_id}/{dataset}_model\"\n",
    "        model = mlflow.pytorch.load_model(model_uri)\n",
    "        print(\"No model found in Production stage, using model from latest run for predictions.\")\n",
    "\n",
    "    print(f\"The model's run ID is: {run_id}\")\n",
    "    train_art_uri = mlflow.get_run(run_id).info.artifact_uri\n",
    "    \n",
    "    model_args = mlflow.artifacts.load_dict(train_art_uri+\"/config.txt\")\n",
    "\n",
    "    window_size = model_args['window_size']\n",
    "\n",
    "    # --------------------------- START PREDICTION -----------------------------\n",
    "    # Get data from the dataset\n",
    "    (x_new, _) = get_data(dataset, mode=\"new\", start=eval_start, end=eval_end)\n",
    "\n",
    "    # This workaround need sto happen internally at the moment\n",
    "    # We must use the last window_size timestamps from training as the first window_size timestamps\n",
    "    # for evaluation, due to the sliding window framework\n",
    "    x_train, _ = get_data(dataset, mode=\"train\", start=-window_size, end=None)\n",
    "    x_new = np.concatenate((x_train, x_new), axis=0)\n",
    "\n",
    "    # Cast data into tensor objects\n",
    "    x_new = torch.from_numpy(x_new).float()\n",
    "    n_features = x_new.shape[1]\n",
    "\n",
    "    # We want to perform forecasting/reconstruction on all features\n",
    "    out_dim = n_features\n",
    "\n",
    "    # Construct dataset from tensor objects - no stride here\n",
    "    new_dataset = SlidingWindowDataset(x_new, window_size)\n",
    "\n",
    "    print(\"Predicting:\")\n",
    "    # Create the data loader - no shuffling here\n",
    "    new_loader, _ = create_data_loader(new_dataset, model_args['batch_size'], None, False)\n",
    "\n",
    "    # Initialize the Handler module\n",
    "    handler = Handler(\n",
    "        model=model,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        window_size=window_size,\n",
    "        n_features=n_features,\n",
    "        batch_size=model_args['batch_size'],\n",
    "        n_epochs=None,\n",
    "        patience=None,\n",
    "        forecast_criterion=None,\n",
    "        recon_criterion=None,\n",
    "        use_cuda=use_cuda,\n",
    "        print_every=None,\n",
    "        gamma=model_args['gamma']\n",
    "    )\n",
    "\n",
    "    # Get new scores (inference needs to be fast, no details needed)\n",
    "    print(\"Calculating scores on new data...\")\n",
    "    new_scores = handler.score(loader=new_loader, details=False)\n",
    "\n",
    "    # Calculate threshold via POT based on the new_scores\n",
    "    if threshold_type == \"POT\":\n",
    "        # Load stored scores for training data\n",
    "        print(\"Loading scores from data used for training...\")\n",
    "        train_scores = json_to_numpy(train_art_uri+\"/anom_scores.json\")\n",
    "        \n",
    "        if use_mov_av:\n",
    "            smoothing_window = int(model_args['batch_size'] * window_size * 0.05)\n",
    "            train_scores = pd.DataFrame(train_scores).ewm(span=smoothing_window).mean().values.flatten()\n",
    "            new_scores = pd.DataFrame(new_scores).ewm(span=smoothing_window).mean().values.flatten()\n",
    "\n",
    "        pot_thresh = pot_threshold(train_scores, new_scores, q=q, level=level, dynamic=dynamic_pot)\n",
    "\n",
    "        # Log the POT threshold as part of this run, do not override anything from training/eval\n",
    "        mlflow.log_dict({\"POT\": pot_thresh}, \"thresholds.json\")\n",
    "\n",
    "        threshold = pot_thresh\n",
    "    # Pick among the selected thresholds\n",
    "    else:\n",
    "        thresholds = mlflow.artifacts.load_dict(train_art_uri+\"/thresholds.json\")\n",
    "        threshold = thresholds[\"epsilon\"]\n",
    "\n",
    "    print(f\"Predicting anomalies based on {threshold_type}-generated threshold - threshold value: {threshold}\")\n",
    "\n",
    "    # Make predictions based on threshold\n",
    "    anomalies = handler.predict(new_scores, threshold)\n",
    "    \n",
    "    # ---------------------------- END PREDICTION ------------------------------\n",
    "\n",
    "    # save results\n",
    "    with open('anomalies.txt', 'w') as f:\n",
    "        for anom in anomalies:\n",
    "            f.write(f\"{anom}\\n\")\n",
    "    mlflow.log_artifact('anomalies.txt')\n",
    "    os.remove('anomalies.txt')\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f428e2c-1e5c-4780-ac77-6bac788241bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "dataset",
      "width": 88
     },
     {
      "breakBefore": false,
      "name": "eval_start",
      "width": 88
     },
     {
      "breakBefore": false,
      "name": "eval_end",
      "width": 88
     },
     {
      "breakBefore": false,
      "name": "threshold_type",
      "width": 132
     },
     {
      "breakBefore": false,
      "name": "use_cuda",
      "width": 88
     },
     {
      "breakBefore": false,
      "name": "use_mov_av",
      "width": 88
     },
     {
      "breakBefore": false,
      "name": "q",
      "width": 88
     },
     {
      "breakBefore": false,
      "name": "level",
      "width": 88
     },
     {
      "breakBefore": false,
      "name": "dynamic_pot",
      "width": 132
     }
    ]
   },
   "notebookName": "predict",
   "widgets": {
    "dataset": {
     "currentValue": "system_1",
     "nuid": "e9c16260-e768-45d7-aa37-19a5928486ac",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "system_1",
      "label": null,
      "name": "dataset",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "dynamic_pot": {
     "currentValue": "False",
     "nuid": "e86196cb-56e2-444a-b17b-bf31f45bb663",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "False",
      "label": null,
      "name": "dynamic_pot",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "eval_end": {
     "currentValue": "None",
     "nuid": "a2479a96-fc93-4998-93b8-0e6181f99593",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "None",
      "label": null,
      "name": "eval_end",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "eval_start": {
     "currentValue": "0",
     "nuid": "0c15166c-a7a5-46af-86b9-496340d04a65",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0",
      "label": null,
      "name": "eval_start",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "level": {
     "currentValue": "0.99",
     "nuid": "1a5279bb-c589-4aaa-b0d3-76372b9585e1",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.99",
      "label": null,
      "name": "level",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "q": {
     "currentValue": "0.001",
     "nuid": "2dfe5c38-8a7a-4394-b058-b5d5c5697358",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.001",
      "label": null,
      "name": "q",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "threshold_type": {
     "currentValue": "POT",
     "nuid": "1dbf1e3c-0c55-43a2-b131-8be0f6cba5ef",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "POT",
      "label": null,
      "name": "threshold_type",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "use_cuda": {
     "currentValue": "True",
     "nuid": "2db40dde-c4af-40b6-b117-078f01b31e33",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "True",
      "label": null,
      "name": "use_cuda",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "use_mov_av": {
     "currentValue": "False",
     "nuid": "3b1b7c23-0d62-4839-b365-8cecd9f54f54",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "False",
      "label": null,
      "name": "use_mov_av",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
